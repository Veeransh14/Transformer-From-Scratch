# Transformer From Scratch 


# Course 1-
# Week 4-

# Forward Propogation and Back Propogation
We have understood both forward and backpropogation in detail in previous chapters uploaded on github.
to summarize we can sat the input given by user is passed and processed through several hidden layers and each of them is randomly initialized certain weight this is random initialization.Once this input is processed it gives us some output.The actual output is compared with the required output and with help of a loss function we calculate error this error helps us reallocate the randomly initiallized weights during back propogation.
In this way our Transformer model is trained and adapts to new thing.

# Parameters and Hyperparameters
In simplified words,

Model Parameters are something that a model learns on its own.

For example,

1.Weights or Coefficients of independent variables in Linear regression model.

2.Weights or Coefficients of independent variables.

3.Split points in Decision Tree.
parameters are W,x,a which play important role in learning of our model.

On other hand hyperparameters are are those constants or values provided to our model which are helpful for faster training.This involves optimizing and tuning of parameters.
It is often observed that for different code or algorithm the rate of learning and adpativity differs this is also because of optimizing of these hyperparameters.
These values are experimentally obtained and observed. Models work at best efficiency and most learning rate when these parameters are optimized.

Lets consider an example-
Say you're flipping a coin which lands heads with probability  θ
 . You flip the coin 20 times and get 15 heads. However, instead of estimating  θ
  directly as  15/20=75
 , let's suppose you have additional prior knowledge about what the coin's probability of landing heads is. That is, say you believe that most coins produced in the world will tend to have a probability of 1/2, and you want to explicitly model your uncertainty about this parameter by adding a prior distribution:  p∼Beta(α,β)
  for some  α,β∈R

Then the posterior distribution  p(θ∣15 heads of 20 coin flips)
  can be worked out as  Beta(α+15,β+5)
 . Using the average value of the posterior as your estimate of  θ
 , one obtains

θ=(α+15)/(α+β+20)
 
 Now notice the explicit importance of the hyperparameters  α,β
 . Specifying a prior distribution of  Beta(1,1)
  represents a weak prior belief that the coin has a 1/2 probability of landing heads:  θ≈72.7
 . On the other hand,  Beta(10,10)
  represents a rather strong belief:  θ≈62.5
 . Therefore while hyperparameters are not directly of interest, they can strongly influence the end result of your inference procedure.

v
