{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OERVd5hrOC7c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rDhWI23nOKJt"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4yMrrMtOPev",
        "outputId": "bfe02d01-3d1e-486e-9ccd-8c507f578462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KqD_unT0OSp9"
      },
      "outputs": [],
      "source": [
        "X_train=[]\n",
        "X_test=[]\n",
        "for i in range(train_X.shape[0]):\n",
        "    X_train.append(train_X[i].flatten())\n",
        "for j in range(test_X.shape[0]):\n",
        "    X_test.append(test_X[j].flatten())\n",
        "X_train=np.array(X_train).T\n",
        "X_test=np.array(X_test).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1RqVHiDAOh2U"
      },
      "outputs": [],
      "source": [
        "from scipy import sparse\n",
        "def convert_labels(y, C=4 ):\n",
        "    Y = sparse.coo_matrix((np.ones_like(y),\n",
        "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwQFxrisOnxN",
        "outputId": "779cdbcd-6d34-408d-9665-e14acdc8ae33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 60000)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "Y_train=convert_labels(train_y,10)\n",
        "Y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5gpVbDpOt7m",
        "outputId": "8f1dac8d-1929-4bb4-e495-b4d25e565475"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 1, ..., 0, 0, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "Y_test=convert_labels(test_y,10)\n",
        "Y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ptp8OUnaOwG2"
      },
      "outputs": [],
      "source": [
        "class neural_network(object):\n",
        "    def __init__(self):\n",
        "        self.input_unit = 784\n",
        "        self.hidden_units_1 = 300\n",
        "        self.hidden_units_2 = 100\n",
        "        self.output_class = 10\n",
        "        self.W1 = 0.01*np.random.randn(self.input_unit, self.hidden_units_1)\n",
        "        self.b1 = np.zeros((self.hidden_units_1, 1))\n",
        "        self.W2 = 0.01*np.random.randn(self.hidden_units_1,self.hidden_units_2)\n",
        "        self.b2 = np.zeros((self.hidden_units_2, 1))\n",
        "        self.W3 = 0.01*np.random.randn(self.hidden_units_2,self.output_class)\n",
        "        self.b3 = np.zeros((self.output_class, 1))\n",
        "\n",
        "    def softmax(self,Z):\n",
        "        e_Z = np.exp(Z)\n",
        "        A = e_Z / e_Z.sum(axis = 0)\n",
        "        return A\n",
        "\n",
        "    def feed_for_ward(self,X):\n",
        "        #layer1\n",
        "        self.Z1= self.W1.T@X + self.b1\n",
        "        self.A1 =  np.maximum(self.Z1,0) #relu function f'(s)=0 if s<=0 else f'(s)=1\n",
        "        #predict\n",
        "        #layer2\n",
        "        self.Z2= self.W2.T@self.A1 + self.b2\n",
        "        self.A2 =  np.maximum(self.Z2,0) #relu function f'(s)=0 if s<=0 else f'(s)=1\n",
        "        #predict\n",
        "        self.Z3 = self.W3.T@self.A2 +self.b3\n",
        "        self.A3 = self.softmax(self.Z3)\n",
        "        return self.A3\n",
        "\n",
        "    def back_propagation(self,X,Y,eta):\n",
        "        self.N=X.shape[1]\n",
        "\n",
        "        self.E3 = (self.A3 - Y)/self.N\n",
        "        self.dW3 = np.dot(self.A2, self.E3.T)\n",
        "        self.db3 = np.sum(self.E3, axis = 1, keepdims = True)\n",
        "        self.E2 = np.dot(self.W3, self.E3)\n",
        "        self.E2[self.Z2 <= 0] = 0 # gradient of ReLU\n",
        "        self.dW2 = np.dot(self.A1, self.E2.T)\n",
        "        self.db2 = np.sum(self.E2, axis = 1, keepdims = True)\n",
        "        self.E1 = np.dot(self.W2, self.E2)\n",
        "        self.E1[self.Z1 <= 0] = 0 # gradient of ReLU\n",
        "        self.dW1 = np.dot(X, self.E1.T)\n",
        "        self.db1 = np.sum(self.E1, axis = 1, keepdims = True)\n",
        "\n",
        "        # Gradient Descent update\n",
        "        self.W1 += -eta*self.dW1\n",
        "        self.b1 += -eta*self.db1\n",
        "        self.W2 += -eta*self.dW2\n",
        "        self.b2 += -eta*self.db2\n",
        "        self.W3 += -eta*self.dW3\n",
        "        self.b3 += -eta*self.db3\n",
        "\n",
        "    def train(self, X, Y, iteration=100,eta= 0.015):\n",
        "        self.lost_arr = []\n",
        "        for i in range(iteration):\n",
        "            y_hat=self.feed_for_ward(X)\n",
        "            loss = self.cost(Y , y_hat)\n",
        "            self.lost_arr.append(loss)\n",
        "            self.back_propagation(X,Y,eta)\n",
        "            if i%10==0:\n",
        "                print(f\"loss after inter {i}: \", loss)\n",
        "\n",
        "    def cost(self,Y, Yhat):\n",
        "        epsilon = 1e-5\n",
        "        return -np.sum(Y*np.log(Yhat+ epsilon))/Y.shape[1]\n",
        "\n",
        "    def vis_loss(self,inter):\n",
        "        x = np.arange(0,inter)\n",
        "        y=self.lost_arr\n",
        "        plt.plot(x,y,color='green')\n",
        "\n",
        "    def predict(self,X):\n",
        "        y_hat = self.feed_for_ward(X)\n",
        "        p = []\n",
        "        for i in y_hat.T:\n",
        "            temp = np.zeros(self.output_class)\n",
        "            temp[np.where(i==i.max())[0][0]]=1\n",
        "            p.append(temp)\n",
        "        return np.array(p).T\n",
        "\n",
        "    def score(self,predict, y):\n",
        "        cnt=0\n",
        "        for i in range(predict.shape[1]):\n",
        "            if ((predict[:,i]==y[:,i]).all()):\n",
        "                cnt+=1\n",
        "        return round(cnt/predict.shape[1]*100,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAfdig0kO6KI",
        "outputId": "6967159b-ab87-4211-9fdb-2ebb2a1facc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss after inter 0:  2.3241011279217143\n",
            "loss after inter 10:  2.301561821728704\n",
            "loss after inter 20:  0.8896088219484387\n",
            "loss after inter 30:  0.6148059284807955\n",
            "loss after inter 40:  0.48917428398434476\n",
            "loss after inter 50:  0.33409662418584907\n",
            "loss after inter 60:  0.3018917879249289\n",
            "loss after inter 70:  0.3070300953831267\n",
            "loss after inter 80:  0.252621223502583\n",
            "loss after inter 90:  0.23344825665323166\n",
            "loss after inter 100:  0.22760917917627127\n",
            "loss after inter 110:  0.23315378541018267\n",
            "loss after inter 120:  0.19795385643259217\n",
            "loss after inter 130:  0.1868091927122064\n",
            "loss after inter 140:  0.17772312523170314\n",
            "loss after inter 150:  0.16977320694878323\n",
            "loss after inter 160:  0.16263106565387053\n",
            "loss after inter 170:  0.15615390680542424\n",
            "loss after inter 180:  0.15020978777839383\n",
            "loss after inter 190:  0.14472374602281818\n",
            "loss after inter 200:  0.13964532719981493\n",
            "loss after inter 210:  0.1349097174761042\n",
            "loss after inter 220:  0.13049281266572013\n",
            "loss after inter 230:  0.12636292779130737\n",
            "loss after inter 240:  0.12247609954366223\n",
            "loss after inter 250:  0.1188189946211608\n",
            "loss after inter 260:  0.11539306747319827\n",
            "loss after inter 270:  0.11220881053073306\n",
            "loss after inter 280:  0.10924513024813444\n",
            "loss after inter 290:  0.10642921979932553\n",
            "loss after inter 300:  0.10362134540137295\n",
            "loss after inter 310:  0.10084016528676705\n",
            "loss after inter 320:  0.09808543897014292\n",
            "loss after inter 330:  0.09543422888594888\n",
            "loss after inter 340:  0.09292051537219233\n",
            "loss after inter 350:  0.09052468900658878\n",
            "loss after inter 360:  0.08825429222127151\n",
            "loss after inter 370:  0.08613813007372856\n",
            "loss after inter 380:  0.08411509400048046\n",
            "loss after inter 390:  0.0821610897967414\n",
            "loss after inter 400:  0.08029339879871547\n",
            "loss after inter 410:  0.07852723808635993\n",
            "loss after inter 420:  0.07681502479752209\n",
            "loss after inter 430:  0.07511725362939227\n",
            "loss after inter 440:  0.07348322660164568\n",
            "loss after inter 450:  0.07188107135952557\n",
            "loss after inter 460:  0.07030219836228264\n",
            "loss after inter 470:  0.06874112399544363\n",
            "loss after inter 480:  0.06721964719618068\n",
            "loss after inter 490:  0.06577238001626322\n",
            "loss after inter 500:  0.0644008484997795\n",
            "loss after inter 510:  0.0630665851109638\n",
            "loss after inter 520:  0.061740201663056826\n",
            "loss after inter 530:  0.06044355703336916\n",
            "loss after inter 540:  0.0592423966531897\n",
            "loss after inter 550:  0.05812684879030013\n",
            "loss after inter 560:  0.05706596441053178\n",
            "loss after inter 570:  0.05604821615384452\n",
            "loss after inter 580:  0.055047844775612344\n",
            "loss after inter 590:  0.053974347503377255\n",
            "loss after inter 600:  0.05283053169047584\n",
            "loss after inter 610:  0.051634395100259696\n",
            "loss after inter 620:  0.050488816114161346\n",
            "loss after inter 630:  0.049414985875988665\n",
            "loss after inter 640:  0.0484315838817305\n",
            "loss after inter 650:  0.047572193394284874\n",
            "loss after inter 660:  0.046800954462840196\n",
            "loss after inter 670:  0.04606013432655388\n",
            "loss after inter 680:  0.045342618311841616\n",
            "loss after inter 690:  0.044563956101073915\n"
          ]
        }
      ],
      "source": [
        "two_lay = neural_network()\n",
        "two_lay.train(X_train,Y_train,700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "feUAtCvKPDNN"
      },
      "outputs": [],
      "source": [
        "l_pred=two_lay.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "okhD4YRjPFHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d36f86c-bba0-456d-f790-6eb073459912"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97.56"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "two_lay.score(l_pred,Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1mox1LrSgze"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}